{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "import csv\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data...\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "print(\"Reading data...\")\n",
    "import os   \n",
    "userhome = os.path.expanduser('~')\n",
    "csvfile= userhome + r'\\Desktop\\UCSD local\\Web Mining\\Assignment 2\\flights.csv'\n",
    "#dataFile = open(csvfile)\n",
    "lines= []\n",
    "with open(csvfile, \"r\") as f:\n",
    "    cr = csv.DictReader(f, delimiter=',')\n",
    "    for row in cr:\n",
    "        lines.append(row)\n",
    "#header = dataFile.readline()\n",
    "#fields = [\"constant\"] + header.stridatp().replace('\"','').split(';')\n",
    "#featureNames = fields[:-1]\n",
    "#labelName = fields[-1]\n",
    "#lines = [[1.0] + [x for x in l.split(',')] for l in dataFile]\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Randomizing samples\n",
    "ind = [random.randint(0, len(lines)-1) for i in lines]\n",
    "flights = [lines[ind[i]] for i in ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "flightsSp = flights[:100000]\n",
    "D = 60 # Delay threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "i = 0\n",
    "drop = [0]\n",
    "for f in flightsSp:\n",
    "    if f['ARRIVAL_DELAY'] == '':\n",
    "        drop.append(i)\n",
    "    i = i + 1\n",
    "flightsSp = np.delete(flightsSp, drop, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Airports = np.unique([x['ORIGIN_AIRPORT'] for x in flightsSp])\n",
    "len(Airports)\n",
    "Airlines = np.unique([x['AIRLINE'] for x in flightsSp])\n",
    "len(Airlines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['AA', 'AS', 'B6', 'DL', 'EV', 'F9', 'HA', 'MQ', 'NK', 'OO', 'UA',\n",
       "       'US', 'VX', 'WN'], \n",
       "      dtype='<U2')"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Airlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "flightsTrain = flightsSp[:int(len(flightsSp)/2)]\n",
    "flightsValid = flightsSp[int(len(flightsSp)/2):int(len(flightsSp)/2)+int(len(flightsSp)/4)]\n",
    "flightsTest = flightsSp[int(len(flightsSp)/2)+int(len(flightsSp)/4):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_data = np.array([0.0 if float(f['ARRIVAL_DELAY']) < 0 else 1.0 if float(f['ARRIVAL_DELAY']) >= 0 and float(f['ARRIVAL_DELAY']) < D else 2.0 for f in flightsTrain])\n",
    "y_valid = np.array([0.0 if float(f['ARRIVAL_DELAY']) < 0 else 1.0 if float(f['ARRIVAL_DELAY']) >= 0 and float(f['ARRIVAL_DELAY']) < D else 2.0 for f in flightsValid])\n",
    "y_test = np.array([0.0 if float(f['ARRIVAL_DELAY']) < 0 else 1.0 if float(f['ARRIVAL_DELAY']) >= 0 and float(f['ARRIVAL_DELAY']) < D else 2.0 for f in flightsTest])\n",
    "\n",
    "y_data.shape += (1, )\n",
    "y_valid.shape += (1, )\n",
    "y_test.shape += (1, )\n",
    "\n",
    "globalAverage = float(sum(y_data) * 1.0 / len(y_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def feature(datum):\n",
    "     # One feature dimension per month\n",
    "    monthFeat = [0]*(12 - 1)\n",
    "    if int(datum['MONTH']) != 12:\n",
    "        monthFeat[int(datum['MONTH']) - 1] = 1.0\n",
    "    \n",
    "    f_airlines = [0]*len(Airlines)\n",
    "    f_airlines[airlines_id[datum['AIRLINE']]] = 1    \n",
    "    \n",
    "    if  float(datum['DEPARTURE_DELAY']) < 0:\n",
    "        delayFeat = 0\n",
    "    elif  float(datum['DEPARTURE_DELAY']) >= 0 and  float(datum['DEPARTURE_DELAY']) < D:\n",
    "        delayFeat = 1\n",
    "    elif float(datum['DEPARTURE_DELAY']) >= D:\n",
    "        delayFeat = 2\n",
    "    \n",
    "    feat = [1.0, delayFeat] + monthFeat + f_airlines\n",
    "    return feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "airlines_id = dict(zip(Airlines,range(len(Airlines))))\n",
    "x_data = np.matrix([feature(f) for f in flightsTrain])\n",
    "x_valid = np.matrix([feature(f) for f in flightsValid])\n",
    "x_test = np.matrix([feature(f) for f in flightsTest])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[ 1.,  1.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,\n",
       "          0.]])"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'AIRLINE': 'UA',\n",
       " 'AIRLINE_DELAY': '28',\n",
       " 'AIR_SYSTEM_DELAY': '0',\n",
       " 'AIR_TIME': '149',\n",
       " 'ARRIVAL_DELAY': '28',\n",
       " 'ARRIVAL_TIME': '1532',\n",
       " 'CANCELLATION_REASON': '',\n",
       " 'CANCELLED': '0',\n",
       " 'DAY': '18',\n",
       " 'DAY_OF_WEEK': '4',\n",
       " 'DEPARTURE_DELAY': '42',\n",
       " 'DEPARTURE_TIME': '1248',\n",
       " 'DESTINATION_AIRPORT': 'FLL',\n",
       " 'DISTANCE': '1065',\n",
       " 'DIVERTED': '0',\n",
       " 'ELAPSED_TIME': '164',\n",
       " 'FLIGHT_NUMBER': '1601',\n",
       " 'LATE_AIRCRAFT_DELAY': '0',\n",
       " 'MONTH': '6',\n",
       " 'ORIGIN_AIRPORT': 'EWR',\n",
       " 'SCHEDULED_ARRIVAL': '1504',\n",
       " 'SCHEDULED_DEPARTURE': '1206',\n",
       " 'SCHEDULED_TIME': '178',\n",
       " 'SECURITY_DELAY': '0',\n",
       " 'TAIL_NUMBER': 'N73256',\n",
       " 'TAXI_IN': '3',\n",
       " 'TAXI_OUT': '12',\n",
       " 'WEATHER_DELAY': '0',\n",
       " 'WHEELS_OFF': '1300',\n",
       " 'WHEELS_ON': '1529',\n",
       " 'YEAR': '2015'}"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flightsTrain[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n, p = x_data.shape\n",
    "k = 10\n",
    "\n",
    "X = tf.placeholder('float', shape=[n,p])\n",
    "# target vector\n",
    "y = tf.placeholder('float', shape=[n,1])\n",
    "\n",
    "# Create a new variable, and initialize to a vector of zeros\n",
    "beta = tf.Variable(tf.zeros([p]))\n",
    "#betaAp = tf.Variable(tf.constant([0.0]*len(flightsTrain), shape=[len(flightsTrain),1]))\n",
    "alpha = tf.Variable(tf.constant([globalAverage]))\n",
    "#theta = tf.Variable(tf.constant([0.0]*12, shape=[12,1]))\n",
    "\n",
    "# interaction factors, randomly initialized\n",
    "V = tf.Variable(tf.random_normal([k,p], stddev=0.01))\n",
    "\n",
    "# estimate of y, initialized to 0\n",
    "y_hat = tf.Variable(tf.zeros([n,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "linear_terms = tf.add(alpha, tf.reduce_sum(tf.multiply(beta,X),1, keep_dims=True))\n",
    "interactions = (tf.multiply(0.5,tf.reduce_sum(tf.pow( tf.matmul(X, tf.transpose(V)), 2) - tf.matmul(tf.pow(X, 2), tf.transpose(tf.pow(V, 2))),1, keep_dims=True)))\n",
    "y_hat = tf.add(linear_terms, interactions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# L2 regularized sum of squares loss function over beta and V\n",
    "lambda_b = tf.constant(0.001, name='lambda_b')\n",
    "lambda_v = tf.constant(0.001, name='lambda_v')\n",
    "\n",
    "l2_norm = (tf.reduce_sum(\n",
    "            tf.add(\n",
    "                tf.multiply(lambda_b, tf.pow(beta, 2)),\n",
    "                tf.multiply(lambda_v, tf.pow(V, 2)))))\n",
    "\n",
    "error = tf.reduce_mean(tf.square(y - y_hat))\n",
    "loss = tf.add(error, l2_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Stochastic gradient descent\n",
    "eta = tf.constant(0.1)\n",
    "optimizer = tf.train.AdagradOptimizer(eta).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE train:  0.203391\n",
      "Loss (regularized error): 0.207553\n",
      "Predictions: [[ 0.71606487]\n",
      " [ 0.18984789]\n",
      " [ 0.82571912]\n",
      " ..., \n",
      " [ 0.04661187]\n",
      " [ 0.73812878]\n",
      " [ 0.69418246]]\n",
      "Learnt weights: [-0.10597961  0.59478092  0.0192516   0.03382099 -0.00996979 -0.00270214\n",
      " -0.00577645  0.00856004 -0.0006979  -0.02035226 -0.03296826 -0.03086338\n",
      " -0.02586823 -0.02300419 -0.00932483 -0.00610413 -0.0898926   0.04021576\n",
      "  0.04133448  0.02897581  0.00367897  0.04151224  0.03674183 -0.10402381\n",
      "  0.0130885   0.00788708 -0.08777658]\n",
      "Learnt factors: [[  2.68682260e-02   3.23547348e-02   1.19934725e-02  -5.88242011e-03\n",
      "   -1.05991326e-02   5.79003245e-04   5.97141264e-03   1.31906718e-02\n",
      "   -7.36789429e-04  -2.82591325e-03  -1.78468018e-03  -8.90343916e-03\n",
      "    4.98220790e-03   1.16501423e-02  -4.96737985e-03   1.21251866e-03\n",
      "    1.18406340e-02   9.30488389e-03   9.28193424e-03  -2.01758090e-03\n",
      "    8.76545091e-04   2.54426561e-02   1.78961689e-03   5.15784929e-03\n",
      "    5.97201427e-03   1.65909790e-02  -3.85740609e-03]\n",
      " [ -4.06904938e-03  -7.87895452e-03   7.35744881e-03   1.18828118e-02\n",
      "    1.30420551e-02   8.77424888e-03   3.43813677e-03  -7.72505999e-03\n",
      "   -1.22513752e-02  -2.65379511e-02  -1.40284840e-03  -2.01961515e-03\n",
      "    1.99929439e-03   1.17264902e-02  -1.53158382e-02   3.56068509e-03\n",
      "    1.31733192e-03   1.76496096e-02   2.80441879e-03   1.09844888e-02\n",
      "   -4.50479053e-03  -1.67298540e-02  -1.08188139e-02   1.46337161e-02\n",
      "    4.04148083e-03   4.70133295e-04  -2.45697387e-02]\n",
      " [ -7.36128464e-02  -8.22283849e-02  -5.05749416e-03  -1.70929879e-02\n",
      "   -1.81524809e-02  -5.73717989e-04  -3.08761699e-03  -1.20839421e-02\n",
      "   -1.74870398e-02  -1.42354174e-02  -3.58422287e-03  -1.21766643e-03\n",
      "   -8.08867812e-03  -1.97982299e-03  -8.32804944e-03  -6.34088553e-03\n",
      "    1.48909902e-02  -1.81921218e-02   2.32307869e-03  -3.56395775e-03\n",
      "    1.47387374e-03  -1.18815005e-02  -1.22415908e-02  -8.66225269e-03\n",
      "    1.75231311e-03  -4.42701532e-03   6.21808646e-03]\n",
      " [ -9.32185724e-02  -1.09457515e-01  -8.08534399e-03  -2.19804645e-02\n",
      "   -1.58450454e-02   2.96186842e-03  -1.15370108e-02  -1.68959629e-02\n",
      "   -2.49909032e-02   4.09744540e-03  -2.78357021e-03   2.29355111e-03\n",
      "    7.99262337e-03  -3.85343609e-03  -2.49681529e-03  -1.52943172e-02\n",
      "   -2.36885599e-03  -1.74630042e-02  -1.12165622e-02  -1.00001348e-02\n",
      "   -2.29169093e-02  -2.30325758e-02  -3.21669355e-02  -8.93387361e-04\n",
      "    2.47857347e-03  -3.70513368e-03  -9.81914718e-03]\n",
      " [  7.43992766e-03   6.07588701e-03  -5.19587845e-03   1.53267791e-03\n",
      "    1.38598215e-02  -4.78176080e-04   8.78957391e-04  -1.59175117e-02\n",
      "   -7.72563741e-03   1.38501721e-02  -2.51861289e-03  -9.16748866e-03\n",
      "    9.26832855e-03  -1.29521247e-02  -6.88364776e-03   5.16101485e-03\n",
      "    7.44713645e-04   1.31041585e-02  -6.24817982e-03  -5.99392178e-03\n",
      "    9.55199357e-03   7.54504651e-03   2.37878202e-03  -2.71509048e-02\n",
      "    8.01719632e-03   1.24846362e-02   6.29835902e-03]\n",
      " [  6.66334331e-02   7.02567026e-02   1.70664247e-02   3.17485891e-02\n",
      "   -1.67126991e-02  -1.18400371e-02   2.74770218e-03  -2.61220196e-03\n",
      "    1.59399211e-02   4.12344886e-03   9.31976363e-03   2.12396402e-02\n",
      "    2.10449174e-02   1.01480000e-02  -1.44653907e-02   3.29129398e-03\n",
      "   -1.62854027e-02   7.77800661e-03  -6.61784830e-03   1.31205721e-02\n",
      "   -5.53342339e-04  -3.40660592e-03   1.74090080e-02  -1.79003552e-02\n",
      "   -1.40674207e-02   4.50372277e-03   4.42611612e-03]\n",
      " [ -8.78802985e-02  -8.69080946e-02  -6.22306485e-03   1.14374589e-02\n",
      "   -1.00153945e-02  -1.77747644e-02  -2.83830171e-03  -1.18974457e-02\n",
      "   -2.07953658e-02   5.18282410e-03   8.20694771e-03   5.47371153e-03\n",
      "   -2.91517167e-03  -1.10052777e-02   1.00998022e-02  -7.78921973e-03\n",
      "    3.01217530e-02  -1.76507444e-03  -1.19299171e-02  -8.80627427e-03\n",
      "   -9.27950162e-03   3.24153597e-03  -6.17499975e-03   1.30532142e-02\n",
      "   -1.43971574e-02   9.89392959e-03   5.60263451e-03]\n",
      " [  9.41446200e-02   9.50414315e-02   1.50468303e-02  -2.24580406e-03\n",
      "    5.95931429e-03   7.18319230e-03   9.20855161e-03   1.75837483e-02\n",
      "    1.94775162e-03  -5.77521790e-03  -1.01168947e-02   4.84432187e-03\n",
      "    1.34166153e-02   6.30329922e-03   2.48596398e-03  -8.24971311e-03\n",
      "   -2.96688173e-02   1.47062372e-02   2.70497566e-03   3.14621860e-03\n",
      "   -1.60471245e-03  -9.77422111e-03   2.09452361e-02  -1.09513216e-02\n",
      "    1.45363780e-02  -3.78096034e-03  -8.86535738e-03]\n",
      " [  3.75233125e-03   6.38467912e-03  -7.51465652e-03   1.72249619e-02\n",
      "    1.29421037e-02   5.68719441e-03  -1.09541481e-02   8.33709352e-03\n",
      "   -7.98673276e-03   2.17827223e-03   9.65939276e-03   2.40010256e-03\n",
      "    7.04387808e-03   2.72378372e-03  -1.09710507e-02   5.98509656e-03\n",
      "   -6.18615234e-03   2.02139420e-03   4.82547143e-03   6.86432654e-03\n",
      "   -2.89981742e-03  -3.20120039e-03   1.48362499e-02   4.69335169e-03\n",
      "   -2.00691307e-03   1.49936543e-03   2.01525138e-04]\n",
      " [ -5.03202714e-02  -6.15804940e-02   7.06112850e-03  -2.69443262e-02\n",
      "    7.96556380e-03  -1.21964430e-02  -8.16928688e-03  -3.27045843e-02\n",
      "   -7.98222143e-03  -2.14172732e-02  -1.47259999e-02   2.64197728e-03\n",
      "    8.65167845e-03  -7.29518570e-03   4.70726984e-03   9.21966881e-03\n",
      "    2.82679684e-06  -2.00799536e-02  -6.78064767e-03  -3.69258644e-03\n",
      "   -2.14327592e-02  -1.86245609e-02  -4.60552471e-03   1.55634079e-02\n",
      "   -4.34710318e-03  -5.46880346e-03   4.08925256e-03]]\n"
     ]
    }
   ],
   "source": [
    "# that's a lot of iterations\n",
    "N_EPOCHS = 100\n",
    "# Launch the graph.\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    for epoch in range(N_EPOCHS):\n",
    "        #indices = np.arange(n)\n",
    "        #np.random.shuffle(indices)\n",
    "        #x_data, y_data = x_data[indices], y_data[indices]\n",
    "        sess.run(optimizer, feed_dict={X: x_data, y: y_data})\n",
    "\n",
    "    print('MSE train: ', sess.run(error, feed_dict={X: x_data, y: y_data}))\n",
    "    print('Loss (regularized error):', sess.run(loss, feed_dict={X: x_data, y: y_data}))\n",
    "    print('Predictions:', sess.run(y_hat, feed_dict={X: x_data, y: y_data}))\n",
    "    predictions_train = sess.run(y_hat, feed_dict={X: x_data, y: y_data})\n",
    "    print('Learnt weights:', sess.run(beta, feed_dict={X: x_data, y: y_data}))\n",
    "    print('Learnt factors:', sess.run(V, feed_dict={X: x_data, y: y_data}))\n",
    "    \n",
    "    linear_terms = tf.add(alpha, tf.reduce_sum(tf.multiply(beta,x_valid),1, keep_dims=True))\n",
    "    interactions = (tf.multiply(0.5,tf.reduce_sum(tf.pow( tf.matmul(tf.cast(x_valid, tf.float32), tf.transpose(V)), 2) - tf.matmul(tf.cast(tf.pow(x_valid, 2), tf.float32), tf.transpose(tf.pow(V, 2))),1, keep_dims=True)))\n",
    "    predictions_valid = tf.add(linear_terms, interactions)\n",
    "    predictions_valid = [np.round(x) for x in predictions_valid.eval()]\n",
    "    \n",
    "    linear_terms = tf.add(alpha, tf.reduce_sum(tf.multiply(beta,x_test),1, keep_dims=True))\n",
    "    interactions = (tf.multiply(0.5,tf.reduce_sum(tf.pow( tf.matmul(tf.cast(x_test, tf.float32), tf.transpose(V)), 2) - tf.matmul(tf.cast(tf.pow(x_test, 2), tf.float32), tf.transpose(tf.pow(V, 2))),1, keep_dims=True)))\n",
    "    predictions_test = tf.add(linear_terms, interactions)\n",
    "    predictions_test = [np.round(x) for x in predictions_test.eval()]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.71090917])"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_valid = [(a==b) for (a,b) in zip(predictions_valid,y_valid)]\n",
    "acc_valid = sum(correct_valid) * 1.0 / len(correct_valid)\n",
    "acc_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.71398884])"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_train = [(a==b) for (a,b) in zip(predictions_train,y_data)]\n",
    "acc_train = sum(correct_train) * 1.0 / len(correct_train)\n",
    "acc_train\n",
    "correct_test = [(a==b) for (a,b) in zip(predictions_test,y_test)]\n",
    "acc_test = sum(correct_test) * 1.0 / len(correct_test)\n",
    "acc_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.44887377]\n"
     ]
    }
   ],
   "source": [
    "bc = sum([y_test[i]==0 and predictions_test[i]!=0 for i in range(len(y_test))])\n",
    "abc = sum([y_test[i]==0 for i in range(len(y_test))])\n",
    "df = sum([y_test[i]==1 and predictions_test[i]!=1 for i in range(len(y_test))])\n",
    "ndef = sum([y_test[i]==1 for i in range(len(y_test))])\n",
    "gh = sum([y_test[i]==2 and predictions_test[i]!=2 for i in range(len(y_test))])\n",
    "ghi = sum([y_test[i]==2 for i in range(len(y_test))])\n",
    "tri_ber = (bc*1.0/abc+df*1.0/ndef+gh*1.0/ghi)*1.0/3\n",
    "print(tri_ber)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
